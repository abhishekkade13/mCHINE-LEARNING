#Logistic Regression

#iD Array
import numpy as np


arr = np.array([1, 2, 3, 4, 5])


print(arr)
print(arr.shape)

# 2 D Array (Matrix)
import numpy as np


arr = np.array([[1, 2, 3], [4, 5, 6]])


print(arr)
print(arr.shape)
import numpy as np


arr = np.array([[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]])
print(arr)
print(arr.shape)
import numpy as np


arr = np.array([[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]])
print(arr)
print(arr.shape)
print(np.ones(3))
print(np.ones([3,2]))
print(np.zeros(3))
print(np.zeros([3,4]))
import csv 
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd
  
def normalize(X): 
     
    #function to normalize feature matrix, X 
    
    mins = np.min(X, axis = 0) 
    maxs = np.max(X, axis = 0) 
    rng = maxs - mins 
    norm_X = 1 - ((maxs - X)/rng) 
    return norm_X 
  
def logistic_func(beta, X):


    # below is the code for 1/1+e^(-(bo*x1+ b1*x2 +  b1*x3............ ))
    #return  horizontal array


    return 1.0/(1 + np.exp(-np.dot(X, beta.T))) 
  
def log_gradient(beta, X, y):


    #first_calc = y_prediction - y_actual  for all samples
    first_calc = logistic_func(beta, X) - y.reshape(X.shape[0], -1)


    # now in below step we will find the partial derivative
    #final_calc= gradient is (y_prediction - y_actual)*x  for all samples


    final_calc = np.dot(first_calc.T, X) 
    
    return final_calc 
  
def cost_func(beta, X, y):


    #y_prediction=  1/1+e^(-(bo*x1+ b1*x2 +  b1*x3............ )) for all samples
    y_prediction= logistic_func(beta, X) 
    y = np.squeeze(y) 


    # calculate cross entropy cost function for all samples
    cost_function = -(y * np.log(y_prediction)) - ((1 - y) * np.log(1 - y_prediction) ) 
    
    # return the sum of  cost function divided by no. of samples
    return np.mean(cost_function) 
  
def train(X, y, beta, lr=.01, converge_change=.001):
cost_per_iter = list()
    cost = cost_func(beta, X, y)
    cost_per_iter.append(cost)
    print("Cost is:", cost)
    change_cost = 1
    num_iter = 1
      
    while(change_cost > converge_change): 
        old_cost = cost 
        #beta= beta - learning_rate * partial derivative of cost function w.r.t beta
        beta = beta - (lr * log_gradient(beta, X, y))


        # again calculate cost function
        cost = cost_func(beta, X, y) 
        
        # find difference between old cost and new cost 
        #if change is greater than .001 then reiterate 
        change_cost = old_cost - cost
        cost_per_iter.append(change_cost)
        num_iter += 1
      
    return beta, num_iter, cost_per_iter  
   
def pred_values(beta, X):


    pred_prob = logistic_func(beta, X) 
    pred_value = np.where(pred_prob >= .5, 1, 0) 
    return np.squeeze(pred_value) 
  
def plot_reg(X, y, beta):


    # labelled observations 
    x_0 = X[np.where(y == 0.0)] 
    x_1 = X[np.where(y == 1.0)] 
      
    # plotting points with diff color for diff label 
    plt.scatter([x_0[:, 1]], [x_0[:, 2]], c='b', label='y = 0') 
    plt.scatter([x_1[:, 1]], [x_1[:, 2]], c='r', label='y = 1') 
      
    # plotting decision boundary 
    x1 = np.arange(0, 1, 0.1) 
    x2 = -(beta[0,0] + beta[0,1]*x1)/beta[0,2] 
    plt.plot(x1, x2, c='k', label='reg line') 
  
    plt.xlabel('x1') 
    plt.ylabel('x2') 
    plt.legend() 
    plt.show() 
      


if __name__ == "__main__":


    # load the dataset
    dataset = pd.read_csv('/content/dataset1 (2).csv')  
    dataset=np.array(dataset) 
    
    # normalizing feature matrix 
    X = normalize(dataset[:, :-1]) 
    print(X)
      
    # stacking columns wth all ones in feature matrix 
    X = np.hstack((np.matrix(np.ones(X.shape[0])).T, X))
    print('\n')
    print(X)


    # response vector 
    y = dataset[:, -1] 


    # initial beta values 
    beta = np.matrix(np.zeros(X.shape[1])) 
    print('\n')
    print(beta)


    # beta values after running gradient descent 
    beta, num_iter, cost_per_iter = train(X, y, beta)
    itr = np.arange(1,num_iter+1)
    print("\n")
    print("Cost_per_iteration = ", np.array(cost_per_iter).T)
    #plt.plot(cost_per_iter, itr)


    # estimated beta values and number of iterations 
    print('\n')
    print("Estimated regression coefficients:", beta) 
    print("No. of iterations:", num_iter) 
    # predicted labels 
    y_pred = pred_values(beta, X)
      
    # number of correctly predicted labels 
    print("\n")
    print("Correctly predicted labels:", np.sum(y == y_pred))
    
    # plotting regression line 
    plot_reg(X, y, beta)

Let's Try To Plot a Sigmoid

import math


def sigmoid(x):
    a = []
    for item in x:
        a.append(1/(1+math.exp(-item)))
    return a


x = np.arange(-10., 10., 0.2)
sig = sigmoid(x)
plt.plot(x,sig)
plt.show()
